# SML_310_Project
Towards Developing a Robust Deep Learning Based Sign Language Recognition System




In this project, I propose a robust deep learning-based sign language recognition system on two public American Sign Language (ASL) datasets. I built my own deep 71 layers CNN model, Sign_languageNet, which is robust and able to classify statics signs from the ASL alphabet. In addition, for both datasets with this as the baseline model, the state-of-the-art CNN architectures were tested using pretrained weights on large dataset such as ImageNet through transfer learning to see how they affect the performance. Thus, such a system is important as being able to recognize fingerspelling-based hand gestures leads to whole words being recognized through combining signs, allowing for easier communication between ASL and non-ASL speakers. Several techniques were implemented to improve the performance of Sign_languageNet and transfer learning models such as hyperparameter tuning, regularization, data augmentation, and test-time augmentations. For the two public ASL datasets, it is proven that data augmentation and regularization not only helped to build a robust model, but also improved the per class performance of letter. Confusion matrices, accuracies, and F1-scores are used to evaluate and analyze the performance of the classification models and the per class performance of the letters. The results of the models trained on both datasets are promising, and they can be deployed for real time application which would bring a huge impact on ASL translation by facilitating the communication between hearing and hearing-impaired people.
