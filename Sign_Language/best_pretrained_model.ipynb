{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfNy0yWoyWLJ"
      },
      "source": [
        "## Author- Daniel Melesse\n",
        "- SML 310 project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNYv7_Slydjv"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26YGgV6a-lRt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imread\n",
        "import scipy\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.python.framework import ops\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation,RandomCrop, RandomContrast, Resizing,RandomZoom\n",
        "import os\n",
        "%matplotlib inline\n",
        "np.random.seed(1)\n",
        "import numpy as np\n",
        "np.random.seed(5) \n",
        "import tensorflow as tf\n",
        "#tf.set_random_seed(2)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from time import perf_counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.python.framework import ops\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import scipy.misc\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3 \n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Add, Dropout, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "from matplotlib.pyplot import imshow\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5TXXZo5aWmj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from time import perf_counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU59ZlTv45w7"
      },
      "source": [
        "### Set up a working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gik631CT_Txn",
        "outputId": "9eaf6085-9294-4cbc-903b-4d765e6f9911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wc3JLTh4xMP"
      },
      "source": [
        "### Check GPU  and connect to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gomC4Rz_ZKm",
        "outputId": "b46fac76-acf1-49e7-c204-9d50bb7b039e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "/device:GPU:0\n",
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mFdhTAO5DGm"
      },
      "source": [
        "### Define train and test directories and load the data\n",
        "\n",
        "\n",
        "- To make it easier, I created subfolders from 87000 images, and I put 2700 images for testing, and 21600 images for training and validaion. However, later I decided only to use 10368 images for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgN8IleH_dQI"
      },
      "outputs": [],
      "source": [
        "data_list = os.listdir('/content/drive/My Drive/Sign_Language/Dataset_2/ASL_test') \n",
        "train_dir = '/content/drive/My Drive/Sign_Language/Dataset_2/final/final_train/'\n",
        "test_dir='/content/drive/My Drive/Sign_Language/Dataset_2/final/final_final_test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWdguqEw61PZ"
      },
      "outputs": [],
      "source": [
        "directory_train= Path(train_dir)\n",
        "image_paths = list(directory_train.glob(r'**/*.jpg'))\n",
        "directory_test= Path(test_dir)\n",
        "test_paths = list(directory_test.glob(r'**/*.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM68lgXF5hDB"
      },
      "source": [
        "### Explanatory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Y703n62Ifi"
      },
      "source": [
        "- Here, let's create a DataFrame with the path and the labels of the images of ASL dataset II so that we can use datagenerator in order to load the images in batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLt-YhPn6prs",
        "outputId": "b4dfb42e-a69f-446a-db6f-2897d5e2870c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train images in the ASL: 21600\n",
            "\n",
            " The total number of labels is: 27\n",
            "\n",
            "The labels of the dataset are: ['L' 'D' 'U' 'P' 'M' 'F' 'O' 'NOTHING' 'W' 'S' 'DELETE' 'B' 'G' 'C' 'T'\n",
            " 'H' 'K' 'V' 'A' 'Q' 'I' 'R' 'Y' 'N' 'E' 'X' 'SPACE']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def preprocess_image(path):\n",
        "    labels = [str(path[i]).split(\"/\")[-2] \\\n",
        "              for i in range(len(path))]\n",
        "    image_path = pd.Series(path, name='image_path').astype(str)\n",
        "    labels = pd.Series(labels, name='image_label')\n",
        "    df = pd.concat([image_path, labels], axis=1)\n",
        "    # This huffle the DataFrame and reset index\n",
        "    image_data_frame = df.sample(frac=1,random_state=0).reset_index(drop = True)\n",
        "    return image_data_frame\n",
        "\n",
        "image_data_frame = preprocess_image(image_paths)\n",
        "print('The number of train images in the ASL:', image_data_frame.shape[0])\n",
        "print(\"\")\n",
        "print(' The total number of labels is:', len(image_data_frame.image_label.unique()))\n",
        "print(\"\")\n",
        "print('The labels of the dataset are:', image_data_frame.image_label.unique())\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_data_frame.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lkmBWpWK3RKb",
        "outputId": "51f2c233-b6f2-44e3-d5df-7dc1d8ab6012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path image_label\n",
              "0  /content/drive/My Drive/Sign_Language/Dataset_...           L\n",
              "1  /content/drive/My Drive/Sign_Language/Dataset_...           D\n",
              "2  /content/drive/My Drive/Sign_Language/Dataset_...           U\n",
              "3  /content/drive/My Drive/Sign_Language/Dataset_...           P\n",
              "4  /content/drive/My Drive/Sign_Language/Dataset_...           M"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e53c769-769d-47c9-afbb-fa53f1a20c35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>image_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/My Drive/Sign_Language/Dataset_...</td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/My Drive/Sign_Language/Dataset_...</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/My Drive/Sign_Language/Dataset_...</td>\n",
              "      <td>U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/My Drive/Sign_Language/Dataset_...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/My Drive/Sign_Language/Dataset_...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e53c769-769d-47c9-afbb-fa53f1a20c35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7e53c769-769d-47c9-afbb-fa53f1a20c35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7e53c769-769d-47c9-afbb-fa53f1a20c35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuIpphPxZYLI",
        "outputId": "aa87d619-d92e-4a46-a8d9-cb50579eec3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of test images in the ASL dataset II is: 2700\n",
            "\n",
            " The total number of labels is: 27\n"
          ]
        }
      ],
      "source": [
        "df_test = preprocess_image(test_paths)\n",
        "print('The number of test images in the ASL dataset II is:', df_test.shape[0])\n",
        "print(\"\")\n",
        "print(' The total number of labels is:', len(df_test.image_label.unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAL58McL4C6i"
      },
      "source": [
        "- As we can see above, there are 21000 images, however we want 1037 images for training, and there are 27 classes as we want the best pretrained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUpF786B4mKN"
      },
      "source": [
        "#### Display sample images from the dataset with thier label\n",
        "\n",
        "- Here let's plot a few images randomly from the training set\n",
        "\n",
        "- Given, I have 87000 images and the images are colored and have high resolution, I am going to use only a subset of it which is 10368 for training, 1036 for validation, and 2700 for testing. \n",
        "\n",
        "- Now, let's use subset of the training data in order to train pretrain models to select the best pretrained models. Let's use just use 10 of 10368 images which is 1037 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPYbQXhGApsb",
        "outputId": "a9576e7f-a976-4ddb-bdf1-e76ca1179ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1037, 2)\n"
          ]
        }
      ],
      "source": [
        "# Split into training and test datasets\n",
        "train_df, test_df = train_test_split(image_data_frame.sample(frac = 0.06004), test_size=0.2, random_state=0)\n",
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg5ysddj8hSm"
      },
      "source": [
        "#### Progressive loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvLpOgGb8jxP"
      },
      "source": [
        "- Here we define our data generators for training, validation, and test\n",
        "\n",
        "- This saves huge computational cost, I am loading it from the dataframe to save computation time rather than loading it from a directory. It loads images in batch.\n",
        "\n",
        "- Depending on the type of transfer learning I fix the size of the image.\n",
        "\n",
        "- Here we will normalize each image by dividing by 255. In addition, we further split the training set to train and validation. Where the validation set will be 10% of the original training set. The color mode is RGB as we have colored images, plus the batch_size is 32, and the class mode is catagorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfnovRSfA5OJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fde01d2-da27-4330-9626-b51b94bfba70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 934 validated image filenames belonging to 27 classes.\n",
            "Found 103 validated image filenames belonging to 27 classes.\n",
            "Found 260 validated image filenames belonging to 27 classes.\n"
          ]
        }
      ],
      "source": [
        "num_class=len(data_list)\n",
        "batch_size=32\n",
        "train_datagen=ImageDataGenerator(\n",
        "    featurewise_center=False, samplewise_center=False,\n",
        "    featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
        "    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n",
        "    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
        "    channel_shift_range=0.0, fill_mode='nearest', cval=0.0,\n",
        "    horizontal_flip=False, vertical_flip=False, rescale=1.0/255.0,\n",
        "    preprocessing_function=None, data_format=None, validation_split=0.1, dtype=None\n",
        ")\n",
        "\n",
        "test_datagen=ImageDataGenerator(\n",
        "    featurewise_center=False, samplewise_center=False,\n",
        "    featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
        "    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n",
        "    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
        "    channel_shift_range=0.0, fill_mode='nearest', cval=0.0,\n",
        "    horizontal_flip=False, vertical_flip=False, rescale=1/255,\n",
        "    preprocessing_function=None, data_format=None, validation_split=0, dtype=None\n",
        ")\n",
        "train_data = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        x_col='image_path',\n",
        "        y_col='image_label',\n",
        "        target_size=(224, 224),\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=0,\n",
        "        subset='training'\n",
        "    )\n",
        "val_data = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        x_col='image_path',\n",
        "        y_col='image_label',\n",
        "        target_size=(224, 224),\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=0,\n",
        "        subset='validation')\n",
        "test_data = test_datagen.flow_from_dataframe(\n",
        "        dataframe=test_df,\n",
        "        x_col='image_path',\n",
        "        y_col='image_label',\n",
        "        target_size=(224, 224),\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neural Network"
      ],
      "metadata": {
        "id": "Q0hOL9e9PP7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U_pURCv4Wau"
      },
      "outputs": [],
      "source": [
        "#adapted from https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "def random_invert_img(x, p=0.5):\n",
        "    if  tf.random.uniform([]) < p:\n",
        "        x = (255-x)\n",
        "    else:\n",
        "        x\n",
        "    return x\n",
        "\n",
        "\n",
        "def random_invert(factor=0.5):\n",
        "    return layers.Lambda(lambda x: random_invert_img(x, factor))\n",
        "\n",
        "random_invert = random_invert()\n",
        "\n",
        "\n",
        "class RandomInvert(layers.Layer):\n",
        "    def __init__(self, factor=0.5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.factor = factor\n",
        "\n",
        "    def call(self, x):\n",
        "        return random_invert_img(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tgKrehI4TSz"
      },
      "outputs": [],
      "source": [
        "def random_change_bright_img(x):\n",
        "    return tf.image.random_brightness(x, 0.2)\n",
        "\n",
        "def random_change_brightness():\n",
        "    return layers.Lambda(lambda x: random_change_bright_img(x))\n",
        "\n",
        "random_change_brightness = random_change_brightness()\n",
        "\n",
        "\n",
        "class RandomBrightness(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        return random_change_brightness (x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v06eSQ0X3KhP"
      },
      "outputs": [],
      "source": [
        "def data_augmenter():\n",
        "    '''\n",
        "    Sequential model composed of 2 layers\n",
        "    '''\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
        "    data_augmentation.add(RandomRotation(0.1)) \n",
        "    return data_augmentation\n",
        "def data_augmenter2():\n",
        "    '''\n",
        "    Sequential model composed of 3 layers\n",
        "    '''\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal_and_vertical\"))\n",
        "    data_augmentation.add(RandomBrightness())\n",
        "    data_augmentation.add(RandomRotation(0.2)) \n",
        "\n",
        "    return data_augmentation\n",
        "\n",
        "def data_augmenter3():\n",
        "    '''\n",
        "    Sequential model composed of 3 layers\n",
        "    '''\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomRotation(0.3)) \n",
        "    data_augmentation.add(RandomBrightness())\n",
        "    data_augmentation.add(RandomContrast(0.2))\n",
        "    return data_augmentation\n",
        "def data_augmenter4():\n",
        "    '''\n",
        "    Sequential model composed of 4 layers\n",
        "    '''\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
        "    data_augmentation.add(RandomRotation(0.4)) \n",
        "    data_augmentation.add(RandomBrightness())\n",
        "    data_augmentation.add(RandomContrast(0.2))\n",
        "    data_augmentation.add(RandomInvert())\n",
        "\n",
        "    return data_augmentation\n",
        "\n",
        "def data_augmenter5():\n",
        "    '''\n",
        "    Sequential model composed of 2 layers\n",
        "    '''\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
        "    data_augmentation.add(RandomRotation(0.2)) \n",
        "    data_augmentation.add(RandomBrightness())\n",
        "    data_augmentation.add(RandomCrop(20,20))\n",
        "    data_augmentation.add(Resizing(28, 28, interpolation='bilinear', crop_to_aspect_ratio=True)) #resize to 28 x 28\n",
        "\n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qizp2LwVUCpF"
      },
      "outputs": [],
      "source": [
        "def best_pretrained_model(model):\n",
        "    kwargs =    {'input_shape':(224, 224, 3),\n",
        "                'include_top':False,\n",
        "                'weights':'imagenet',\n",
        "                'pooling':'avg'}\n",
        "    pretrained_model = model(**kwargs)\n",
        "    pretrained_model.trainable = False\n",
        "    inputs = pretrained_model.input\n",
        "   # x=data_augmenter3()(inputs) if data augumentation, have to rearrage how we set the input and output from the pretrained model\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
        "    x= Dropout(0.2)(x)\n",
        "    x= BatchNormalization()(x)\n",
        "    x= tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "    x= Dropout(0.2)(x)\n",
        "    x= BatchNormalization()(x)\n",
        "    outputs = tf.keras.layers.Dense(27, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsuSwyt9UQJ_"
      },
      "outputs": [],
      "source": [
        "all_models = {\n",
        "    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n",
        "    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n",
        "    \"EfficientNetB7\": {\"model\":tf.keras.applications.EfficientNetB7, \"perf\":0},\n",
        "    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n",
        "    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0},\n",
        "    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n",
        "    \"ResNet152\": {\"model\":tf.keras.applications.ResNet152, \"perf\":0}}\n",
        "for name, model in all_models.items():\n",
        "    best_models = best_pretrained_model(model['model'])\n",
        "    all_models[name]['model'] = best_models\n",
        "    history = best_models.fit(train_data,validation_data=val_data,epochs=5,verbose=0) \n",
        "    val_acc = history.history['val_accuracy']\n",
        "    all_models[name]['val_acc'] = [round(v,4) for v in val_acc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EytsqesKXSMZ"
      },
      "outputs": [],
      "source": [
        "final = []\n",
        "for name, v in all_models.items():\n",
        "    final.append([ name, all_models[name]['val_acc'][-1]])\n",
        "dataf = pd.DataFrame(final, \n",
        "                          columns = ['model','val_accuracy'])\n",
        "dataf.sort_values(by='val_accuracy', ascending=False, inplace=True)\n",
        "dataf.reset_index(inplace=True,drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEMB2qpqaBr-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,5))\n",
        "sns.barplot(x = 'model', y = 'val_accuracy', data = dataf)\n",
        "plt.title('Accuracy on the validation set for 5 epoch)', fontsize = 15)\n",
        "plt.ylim(0,1)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gMgcTyxCO1eN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "best_pretrained_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}